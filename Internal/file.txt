
curl -o notes.txt https://tinyurl.com/sanjayml


DE.TXT : All 4 Weeks
https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/DE.TXT

week 5 : Linear Regression on CSV Data
https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/5%20-Linear%20Regression%20on%20CSV%20Data.txt


week 6 : KNN CLASSIFIER (with prediction report)
https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/6%20-%20KNN%20CLASSIFIER%20(with%20prediction%20report).txt


week 7 : ID3 Algorithm-Decision Tree
https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/7%20-%20ID3%20Algorithm-Decision%20Tree.txt

week 8 : feature selection and extraction techniques (LDA & PCA) using Python.
https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/8%20-%20feature%20selection%20and%20extraction%20techniques.txt

WEEK 9: Naive Bayes Classifier (Full Code with Accuracy)
https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/9%20-%20Naive%20Bayes%20Classifier.txt

Week 10 : complete Python program to implement Support Vector Machine (SVM) for binary classification using a CSV file.
https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/10%20-%20Support%20Vector%20Machine%20(SVM).txt

Week 11 : code to implement Regularized Logistic Regression using a sample training dataset from a CSV file.
https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/11%20-%20Regularized%20Logistic%20Regression.txt

WEEK 12: Ensemble Learning (Bagging, Boosting, Random Forest)
https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/12%20-%20Ensemble%20Learning.txt


WEEK 13:
i. Write a program to perform Clustering using K-means after applying PCA and Determining the value of K using the Elbow method.
https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/13%20-%20K-means%20after%20applying%20PCA.txt


ii. Write a program to build a model to perform hierarchical clustering.like i need in detailed in depth please
https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/13%20-%20hierarchical%20clustering.txt

Week13_AB.ipynb
https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/Week13_AB.ipynb



WEEK 14:
i. Write a program to implement the Perceptron Learning algorithm for binary classification on the iris dataset.
https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/14-Perceptron%20Learning%20algorithm.txt

ii. Write a program to demonstrate a simple neural network that has one input layer,one hidden layer, and one output layer, which includes the forward and backward propagation algorithms for training the neural network
https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/14-simple%20neural%20network.txt

Week_14A.ipynb
https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/Week_14A.ipynb


viva Q&a

https://raw.githubusercontent.com/sanjayattelli29/ML-All-Weeks/master/Internal/viva.txt



week 14 
from random import seed, random
import numpy as np

def init_net(n_in, n_hid, n_out):
    net = []
    net.append([{'w':[random() for _ in range(n_in + 1)]} for _ in range(n_hid)])
    net.append([{'w':[random() for _ in range(n_hid + 1)]} for _ in range(n_out)])
    return net

def act(w, x):
    a = w[-1]
    for i in range(len(w)-1): a += w[i] * x[i]
    return a

def sig(x): return 1 / (1 + np.exp(-x))
def dsig(o): return o * (1 - o)

def fwd_prop(net, row):
    inp = row
    for layer in net:
        out = []
        for n in layer:
            a = act(n['w'], inp)
            n['out'] = sig(a)
            out.append(n['out'])
        inp = out
    return inp

def bwd_prop(net, exp):
    for i in reversed(range(len(net))):
        layer = net[i]
        err = []
        if i != len(net)-1:
            for j in range(len(layer)):
                e = sum(n['w'][j] * n['delta'] for n in net[i+1])
                err.append(e)
        else:
            for j in range(len(layer)):
                n = layer[j]
                err.append(n['out'] - exp[j])
        for j in range(len(layer)):
            n = layer[j]
            n['delta'] = err[j] * dsig(n['out'])

def upd_wts(net, row, lr):
    for i in range(len(net)):
        inp = row[:-1] if i == 0 else [n['out'] for n in net[i-1]]
        for n in net[i]:
            for j in range(len(inp)):
                n['w'][j] -= lr * n['delta'] * inp[j]
            n['w'][-1] -= lr * n['delta']

def train(net, data, lr, epochs, n_out):
    for ep in range(epochs):
        s_err = 0
        for row in data:
            out = fwd_prop(net, row)
            exp = [0 for _ in range(n_out)]
            exp[row[-1]] = 1
            s_err += sum((exp[i]-out[i])**2 for i in range(len(exp)))
            bwd_prop(net, exp)
            upd_wts(net, row, lr)
        print(f'>epoch={ep}, lrate={lr:.3f}, error={s_err:.3f}')

# --- Main ---
seed(1)
data = [
 [2.78,2.55,0],[1.46,2.36,0],[3.39,4.4,0],[1.38,1.85,0],[3.06,3.0,0],
 [7.62,2.75,1],[5.33,2.08,1],[6.92,1.77,1],[8.67,-0.24,1],[7.67,3.5,1]
]
n_in = len(data[0])-1
n_out = len(set(r[-1] for r in data))
net = init_net(n_in, 2, n_out)
train(net, data, 0.5, 20, n_out)
for l in net: print(l)
